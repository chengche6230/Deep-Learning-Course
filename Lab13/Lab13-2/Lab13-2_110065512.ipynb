{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "        # Select GPU number 1\n",
    "        # tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 15 23:53:42 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 79%   73C    P8    28W / 250W |     21MiB / 11176MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   49C    P2    61W / 250W |    267MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1177      G   /usr/lib/xorg/Xorg                 16MiB |\n",
      "|    1   N/A  N/A     24014      C   ...evin/anaconda3/bin/python      263MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTION_FILE = './words_captcha/spec_train_val.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataset():\n",
    "    caption = []\n",
    "    with open(CAPTION_FILE) as f:\n",
    "        for line in f.readlines():\n",
    "            word = line.split()[1]\n",
    "            caption.append([w for w in word])\n",
    "    target = []\n",
    "    for cap in caption:\n",
    "        tmp = ''\n",
    "        for i in range(len(cap)):\n",
    "            tmp += f'{cap[i]} '\n",
    "        target.append(f'<start> {tmp}<end>')\n",
    "    return target\n",
    "\n",
    "target = readDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagePath():\n",
    "    img_name = []\n",
    "    for i in range(140000):\n",
    "        img_name.append(f'./words_captcha/a{i}.png')\n",
    "    return img_name\n",
    "\n",
    "img_name_vec = imagePath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> t h u s <end>', '<start> w w w <end>', '<start> t i e d <end>', '<start> i d s <end>', '<start> j a m <end>']\n",
      "['./words_captcha/a0.png', './words_captcha/a1.png', './words_captcha/a2.png', './words_captcha/a3.png', './words_captcha/a4.png']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120000, 140000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(target[:5])\n",
    "print(img_name_vec[:5])\n",
    "len(target), len(img_name_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 20000, 20000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_train, target_val = target[:100000], target[100000:]\n",
    "img_train, img_val = img_name_vec[:100000], img_name_vec[100000: 120000]\n",
    "img_test = img_name_vec[120000:]\n",
    "len(img_train), len(img_val), len(img_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and tokenize the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(target)\n",
    "train_seqs = tokenizer.texts_to_sequences(target)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(target)\n",
    "\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<start> t h u s <end>',\n",
       " [2, 9, 18, 17, 6, 3],\n",
       " array([ 2,  9, 18, 17,  6,  3,  0], dtype=int32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cap_vector)\n",
    "target[0], train_seqs[0], cap_vector[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: '\\<start\\>': 2, '\\<end\\>': 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 20000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_train, cap_val = cap_vector[:100000], cap_vector[100000:]\n",
    "len(cap_train), len(cap_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 # batch_size=32 occupy about 9G memory\n",
    "BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(img_train) // BATCH_SIZE\n",
    "\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_fuc(image_path, annotation):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # resize height to 300, width to 160\n",
    "    img = tf.image.resize(img, (300, 160))\n",
    "    img = img / 255 - 1.\n",
    "    return img, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and valodation dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((img_train, cap_train))\n",
    "train_dataset = train_dataset.map(\n",
    "        map_fuc, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((img_val, cap_val))\n",
    "val_dataset = val_dataset.map(\n",
    "        map_fuc, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_leaky_relu(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, size, stride):\n",
    "        super(conv_leaky_relu, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters, size, stride, padding=\"same\",\n",
    "                      kernel_initializer=tf.keras.initializers.TruncatedNormal())\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.lkrelu = tf.keras.layers.LeakyReLU(0.1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.batchnorm(x,training = training)\n",
    "        x = self.lkrelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use VGG19 as feature extracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG19\n",
    "class Feature_Extracter(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Feature_Extracter, self).__init__()\n",
    "        self.cr1 = conv_leaky_relu(64, 3, 1)\n",
    "        self.cr2 = conv_leaky_relu(64, 3, 1)\n",
    "        self.max_pooling1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr3 = conv_leaky_relu(128, 3, 1)\n",
    "        self.cr4 = conv_leaky_relu(128, 3, 1)\n",
    "        self.max_pooling2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr5 = conv_leaky_relu(256, 3, 1)\n",
    "        self.cr6 = conv_leaky_relu(256, 3, 1)\n",
    "        self.cr7 = conv_leaky_relu(256, 3, 1)\n",
    "        self.cr8 = conv_leaky_relu(256, 3, 1)\n",
    "        self.max_pooling3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr9 = conv_leaky_relu(512, 3, 1)\n",
    "        self.cr10 = conv_leaky_relu(512, 3, 1)\n",
    "        self.cr11 = conv_leaky_relu(512, 3, 1)\n",
    "        self.cr12 = conv_leaky_relu(512, 3, 1)\n",
    "        self.max_pooling4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr13 = conv_leaky_relu(512, 3, 1)\n",
    "        self.cr14 = conv_leaky_relu(512, 3, 1)\n",
    "        self.cr15 = conv_leaky_relu(512, 3, 1)\n",
    "        self.cr16 = conv_leaky_relu(512, 3, 1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.cr1(inputs,training)\n",
    "        x = self.cr2(x,training)\n",
    "        x = self.max_pooling1(x)\n",
    "        x = self.cr3(x,training)\n",
    "        x = self.cr4(x,training)\n",
    "        x = self.max_pooling2(x)\n",
    "        x = self.cr5(x,training)\n",
    "        x = self.cr6(x,training)\n",
    "        x = self.cr7(x,training)\n",
    "        x = self.cr8(x,training)\n",
    "        x = self.max_pooling3(x)\n",
    "        x = self.cr9(x,training)\n",
    "        x = self.cr10(x,training)\n",
    "        x = self.cr11(x,training)\n",
    "        x = self.cr12(x,training)\n",
    "        x = self.max_pooling4(x)\n",
    "        x = self.cr13(x,training)\n",
    "        x = self.cr14(x,training)\n",
    "        x = self.cr15(x,training)\n",
    "        x = self.cr16(x,training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature__extracter\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_leaky_relu (conv_leaky  multiple                 2048      \n",
      " _relu)                                                          \n",
      "                                                                 \n",
      " conv_leaky_relu_1 (conv_lea  multiple                 37184     \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_leaky_relu_2 (conv_lea  multiple                 74368     \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_3 (conv_lea  multiple                 148096    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_4 (conv_lea  multiple                 296192    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_5 (conv_lea  multiple                 591104    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_6 (conv_lea  multiple                 591104    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_7 (conv_lea  multiple                 591104    \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_8 (conv_lea  multiple                 1182208   \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_9 (conv_lea  multiple                 2361856   \n",
      " ky_relu)                                                        \n",
      "                                                                 \n",
      " conv_leaky_relu_10 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_11 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_12 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_13 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_14 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_15 (conv_le  multiple                 2361856   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,046,400\n",
      "Trainable params: 20,035,392\n",
      "Non-trainable params: 11,008\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extracter = Feature_Extracter()\n",
    "feature_extracter.build((None, 300, 160, 3))\n",
    "feature_extracter.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/vgg19\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "print(start_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Instead of saving image features as numpy file, I modified pipeline a little to achieve end-to-end training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = feature_extracter(img_tensor, True)\n",
    "        features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "    trainable_variables = feature_extracter.trainable_variables + encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.385035\n",
      "Time taken for 1 epoch 1026.052608013153 sec\n",
      "\n",
      "Epoch 2 Loss 0.184486\n",
      "Time taken for 1 epoch 1018.3076481819153 sec\n",
      "\n",
      "Epoch 3 Loss 0.032097\n",
      "Time taken for 1 epoch 1017.7049744129181 sec\n",
      "\n",
      "Epoch 4 Loss 0.019288\n",
      "Time taken for 1 epoch 1018.9943478107452 sec\n",
      "\n",
      "Epoch 5 Loss 0.015148\n",
      "Time taken for 1 epoch 1018.0390944480896 sec\n",
      "\n",
      "Epoch 6 Loss 0.011030\n",
      "Time taken for 1 epoch 1018.7507247924805 sec\n",
      "\n",
      "Epoch 7 Loss 0.009985\n",
      "Time taken for 1 epoch 1018.4691143035889 sec\n",
      "\n",
      "Epoch 8 Loss 0.008267\n",
      "Time taken for 1 epoch 1018.4540584087372 sec\n",
      "\n",
      "Epoch 9 Loss 0.009373\n",
      "Time taken for 1 epoch 1018.5190284252167 sec\n",
      "\n",
      "Epoch 10 Loss 0.007217\n",
      "Time taken for 1 epoch 1019.4814200401306 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    hidden = decoder.reset_state(batch_size=BATCH_SIZE)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    features = feature_extracter(image,False)\n",
    "    features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "\n",
    "    result = np.full((BATCH_SIZE, 1), 2) # full with '<start>'\n",
    "    for i in range(1, max_length):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "        result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(pred):\n",
    "    pred_str = []\n",
    "    for i in range(len(pred)):\n",
    "        tmpstr = ''\n",
    "        for j in range(1, len(pred[i])):\n",
    "            if pred[i][j] == 3: # encounter '<end>'\n",
    "                break\n",
    "            tmpstr += tokenizer.index_word[pred[i][j]]\n",
    "        pred_str.append(tmpstr)\n",
    "    return pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625, Val. Accuracy: 0.9958533493589743\r"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "for (batch, (img_tensor, target)) in enumerate(val_dataset):\n",
    "    pred = evaluate(img_tensor)\n",
    "    pred_str = post_processing(pred)\n",
    "    target_str = post_processing(target.numpy())\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        if pred_str[i] != target_str[i]:\n",
    "            error += 1\n",
    "    \n",
    "    print(f'{batch+1:2d}, Val. Accuracy: {1 - float(error) / ((batch+1)*BATCH_SIZE)}', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_test(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # resize height to 300, width to 160\n",
    "    img = tf.image.resize(img, (300, 160))\n",
    "    img = img / 255 - 1.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((img_test))\n",
    "test_dataset = test_dataset.map(\n",
    "        map_test, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Lab13-2_110065512.txt', 'w') as f:\n",
    "    for (batch, (img_tensor)) in enumerate(test_dataset):\n",
    "        pred = evaluate(img_tensor)\n",
    "        pred_str = post_processing(pred)\n",
    "        for i in range(len(pred_str)):\n",
    "            line = f'a{120000 + batch*BATCH_SIZE + i} {pred_str[i]}\\n'\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "* Overall pipeline: Feature Extracter(VGG19) -> Encoder -> Decoder(GRU)\n",
    "* I tried using pre-trained model, i.e. VGG19 and InceptionV3, as feature extracter but got poor performance.\n",
    "* Validation accuracy: 99.59%"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c5b210ffa015f2312f69f2248e3163602cc860559c1494ea467ed1fecf0f25e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
